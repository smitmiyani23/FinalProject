[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Perform Exploratory Data Analysis with Diabetes Data set before modelling."
  },
  {
    "objectID": "EDA.html#selected-variables",
    "href": "EDA.html#selected-variables",
    "title": "EDA",
    "section": "Selected Variables",
    "text": "Selected Variables\nThe diabetes_binary_health_indicators_BRFSS2015.csv dataset from Kaggle contains health-related data collected from the Behavioral Risk Factor Surveillance System (BRFSS) 2015 survey. This dataset is used to analyze various health indicators and their relationship to diabetes. The response variable, Diabetes_binary, indicates whether a respondent has diabetes. Below is a brief description of each variable in the dataset used for modelling:\n\nDiabetes_binary: The Response Binary variable indicating whether the respondent has diabetes (1) or not (0).\nHighBP: Binary variable indicating whether the respondent has high blood pressure (1) or not (0).\nHighChol: Binary variable indicating whether the respondent has high cholesterol (1) or not (0).\nBMI: Continuous variable representing the Body Mass Index (BMI) of the respondent.\nSmoker: Binary variable indicating whether the respondent has smoked at least 100 cigarettes in their lifetime (1) or not (0).\nStroke: Binary variable indicating whether the respondent has had a stroke (1) or not (0).\nHeartDiseaseorAttack: Binary variable indicating whether the respondent has had coronary heart disease or a myocardial infarction (1) or not (0).\nPhysActivity: Binary variable indicating whether the respondent has engaged in physical activity (excluding their regular job) in the past 30 days (1) or not (0).\nFruits: Binary variable indicating whether the respondent consumes fruit at least once per day (1) or not (0).\nVeggies: Binary variable indicating whether the respondent consumes vegetables at least once per day (1) or not (0).\nHvyAlcoholConsump: Binary variable indicating heavy alcohol consumption, defined as more than 14 drinks per week for men and more than 7 drinks per week for women (1) or not (0).\nAnyHealthcare: Binary variable indicating whether the respondent has any kind of health care coverage (1) or not (0).\nNoDocbcCost: Binary variable indicating whether the respondent could not see a doctor in the past 12 months due to cost (1) or not (0).\nGenHlth: Ordinal variable indicating the respondent’s general health status, with values ranging from 1 (excellent) to 5 (poor).\nDiffWalk: Binary variable indicating whether the respondent has serious difficulty walking or climbing stairs (1) or not (0).\nSex: Binary variable indicating the sex of the respondent (1 for female, 0 for male).\nAge: Ordinal variable indicating the age category of the respondent, with values ranging from 1 (18-24 years) to 13 (80 years or older).\nEducation: Ordinal variable indicating the highest level of education completed by the respondent, with values ranging from 1 (never attended school or only kindergarten) to 6 (college graduate).\nIncome: Ordinal variable indicating the annual household income of the respondent, with values ranging from 1 (less than $10,000) to 8 ($75,000 or more).\n\nThe ultimate goal of modeling this dataset is to develop predictive models that can accurately identify individuals at risk of having diabetes based on various health indicators. In the following sections, we will explore this data and build models to predict the presence of diabetes based on these health indicators."
  },
  {
    "objectID": "EDA.html#exploratory-data-analysis-eda-plan",
    "href": "EDA.html#exploratory-data-analysis-eda-plan",
    "title": "EDA",
    "section": "Exploratory Data Analysis (EDA) Plan",
    "text": "Exploratory Data Analysis (EDA) Plan\nPerforming EDA ensures that we start with a deep understanding of our data, leading to more informed and effective modeling decisions. Specifically, to identify patterns, distribution and insights associated with prediction and response variables, Following EDA will be performed:\n\nInspect the Data -Understand the structure of the dataset, check for missing values, and get an overview of the variables using str() and is.na().\nDescriptive Statistics -Summarize the central tendency, dispersion, and shape of the dataset’s variables using summary().\nDistribution of Target Variable -Understand and explore the binary response variables’ distribution with bar chart to check for class imbalance\nUnivariate Analysis -Understand and explore the prediction variables’ distribution with histograms(for continuous variables) and bar charts (for categorical variables) using ggplot\nBivariate Analysis -Examine relationships between the response variable and each independent variable,r .\nCorrelation Analysis- Examine relationships between the prediction variables with each other via a correlation matrix to check for multicolinearity using cor() from base R."
  },
  {
    "objectID": "EDA.html#importing-packages-and-reading-data",
    "href": "EDA.html#importing-packages-and-reading-data",
    "title": "EDA",
    "section": "Importing Packages and Reading Data",
    "text": "Importing Packages and Reading Data\nImporting necessary packages for EDA\n\n#Packages\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(reshape2)\nlibrary(caret)\nlibrary(Metrics)\nlibrary(ranger)\n\nLoading the data into diabetes_data object using read_csv()\n\n#reading in data\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n\nLooking at the structure of data using str() to determine what kind of pre-processing is needed\n\nstr(diabetes_data)\n\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_binary = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nRemoving the columns CholCheck, MentHlth and PhysHlth as we wont be using them for modelling using select()\n\ndiabetes_data &lt;- diabetes_data |&gt;\n  select(-c(CholCheck,MentHlth,PhysHlth))\n\nDescriptive Statistics of data using summary(). (This is done before coercing to factors as this will yield useful stats like p_hat and s_error(p_hat)\n\nsummary(diabetes_data)\n\n Diabetes_binary      HighBP         HighChol           BMI       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :12.00  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:24.00  \n Median :0.0000   Median :0.000   Median :0.0000   Median :27.00  \n Mean   :0.1393   Mean   :0.429   Mean   :0.4241   Mean   :28.38  \n 3rd Qu.:0.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:31.00  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :98.00  \n     Smoker           Stroke        HeartDiseaseorAttack  PhysActivity   \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000      Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000      1st Qu.:1.0000  \n Median :0.0000   Median :0.00000   Median :0.00000      Median :1.0000  \n Mean   :0.4432   Mean   :0.04057   Mean   :0.09419      Mean   :0.7565  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000      3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000      Max.   :1.0000  \n     Fruits          Veggies       HvyAlcoholConsump AnyHealthcare   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000    Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000    1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :0.0000    Median :1.0000  \n Mean   :0.6343   Mean   :0.8114   Mean   :0.0562    Mean   :0.9511  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000    3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000    Max.   :1.0000  \n  NoDocbcCost         GenHlth         DiffWalk           Sex        \n Min.   :0.00000   Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.08418   Mean   :2.511   Mean   :0.1682   Mean   :0.4403  \n 3rd Qu.:0.00000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :5.000   Max.   :1.0000   Max.   :1.0000  \n      Age           Education        Income     \n Min.   : 1.000   Min.   :1.00   Min.   :1.000  \n 1st Qu.: 6.000   1st Qu.:4.00   1st Qu.:5.000  \n Median : 8.000   Median :5.00   Median :7.000  \n Mean   : 8.032   Mean   :5.05   Mean   :6.054  \n 3rd Qu.:10.000   3rd Qu.:6.00   3rd Qu.:8.000  \n Max.   :13.000   Max.   :6.00   Max.   :8.000  \n\n\nConverting categorical variables and response variable into factors type using as.factor() within mutate(). To achieve this, we first convert all columns into factor and then convert the numeric variable back to type numeric using as.numeric() in mutate().\n\ndiabetes_processed &lt;- diabetes_data |&gt;\n  #Converts all columns to factors\n  mutate(across(everything(),as.factor)) |&gt;\n  #Converts the only non-categorical variable back to numeric \n  mutate(BMI = as.numeric(BMI))\n\nhead(diabetes_processed)\n\n# A tibble: 6 × 19\n  Diabetes_binary HighBP HighChol   BMI Smoker Stroke HeartDiseaseorAttack\n  &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;               \n1 0               1      1           29 1      0      0                   \n2 0               0      0           14 1      0      0                   \n3 0               1      1           17 0      0      0                   \n4 0               1      0           16 0      0      0                   \n5 0               1      1           13 0      0      0                   \n6 0               1      1           14 1      0      0                   \n# ℹ 12 more variables: PhysActivity &lt;fct&gt;, Fruits &lt;fct&gt;, Veggies &lt;fct&gt;,\n#   HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;, NoDocbcCost &lt;fct&gt;,\n#   GenHlth &lt;fct&gt;, DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;,\n#   Income &lt;fct&gt;\n\n\nChecking for any missing data:\n\n#Missing values in each column\ncolSums(is.na(diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n                 BMI               Smoker               Stroke \n                   0                    0                    0 \nHeartDiseaseorAttack         PhysActivity               Fruits \n                   0                    0                    0 \n             Veggies    HvyAlcoholConsump        AnyHealthcare \n                   0                    0                    0 \n         NoDocbcCost              GenHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0"
  },
  {
    "objectID": "EDA.html#response-variable",
    "href": "EDA.html#response-variable",
    "title": "EDA",
    "section": "Response Variable",
    "text": "Response Variable\nSince the response variable is binary, we can obtain estimates “p_hat” and “q_hat”\n\n#Properly coeerce factors to numeric\nbinary_vec &lt;- data.frame(diabetes_processed$Diabetes_binary)\nbinary_vec$diabetes_processed.Diabetes_binary &lt;- as.numeric(binary_vec$diabetes_processed.Diabetes_binary)\nbinary_vec$bool &lt;- ifelse(\n  binary_vec$diabetes_processed.Diabetes_binary == 1,yes = 0,no = 1 )\n\nhead(binary_vec)\n\n  diabetes_processed.Diabetes_binary bool\n1                                  1    0\n2                                  1    0\n3                                  1    0\n4                                  1    0\n5                                  1    0\n6                                  1    0\n\np_hat &lt;- sum((binary_vec$bool))/length(diabetes_processed$Diabetes_binary)\n\nq_hat &lt;- 1-p_hat\n\npaste(\"p_hat is\", p_hat); paste(\"q_hat is\", q_hat)\n\n[1] \"p_hat is 0.139333017975402\"\n\n\n[1] \"q_hat is 0.860666982024598\"\n\n\nVisualizing the response variable distribution using histogram. This tells us if there is class imbalance\n\nggplot(data = diabetes_processed, aes(x = Diabetes_binary )) +\n  geom_bar(show.legend = TRUE) +\n  xlab(label = \"Diabetes_binary\") +\n  ylab(label = \"Frequency\") +\n  ggtitle(label = \"Diabetes Binary Distribution\")\n\n\n\n\nThere is a class imbalance, therefore using logloss is preferable over accuracy to train models."
  },
  {
    "objectID": "EDA.html#predictor-variables",
    "href": "EDA.html#predictor-variables",
    "title": "EDA",
    "section": "Predictor Variables",
    "text": "Predictor Variables\n\nUnivariate Analysis\nExploring the individual distribution of predictor variables:\n\nGives an overview of the central tendency and variability of the variable\nVisualizing the distribution is crucial for identifying skewness and the presence of any outlines.\nCheck validity of predictor entries (check for non logical entries)\n\n\nReturning mean for each column using colMeans() to yield mean for each predictor. This checks for class imbalance and non logical entries\n\n#Using the numeric Dataset \"Diabetes_Data\"\ndiabetes_data |&gt;\n  select(-Diabetes_binary) |&gt;\n  colMeans()\n\n              HighBP             HighChol                  BMI \n          0.42900110           0.42412094          28.38236361 \n              Smoker               Stroke HeartDiseaseorAttack \n          0.44316856           0.04057080           0.09418559 \n        PhysActivity               Fruits              Veggies \n          0.75654368           0.63425576           0.81141990 \n   HvyAlcoholConsump        AnyHealthcare          NoDocbcCost \n          0.05619678           0.95105251           0.08417692 \n             GenHlth             DiffWalk                  Sex \n          2.51139231           0.16822375           0.44034216 \n                 Age            Education               Income \n          8.03211921           5.05043362           6.05387496 \n\n\nVisualizing distributions of numeric variables (only one present in this case) by faceted histograms. To do this, we use gather() to generate key value pairs: “variable” and “value”. We set x = value and facet using facet_wrap(~variable)\n\n# Reshape the dataframe to long format\ndf_long_num &lt;- gather(diabetes_processed |&gt;\n                    select(-Diabetes_binary) |&gt;    \n                    select(where(is.numeric)), key = \"variable\", value = \"value\")\n\n# Create the combined histogram plot\nggplot(df_long_num, aes(x = value)) +\n  geom_histogram(binwidth = 1) +\n  theme(axis.text.x = element_text(angle = 45,\n                                   vjust = 1,\n                                   hjust = 1)) +\n  facet_wrap(~ variable, scales = \"free\") +\n  ggtitle(\"Histograms of all Numeric columns\")\n\n\n\n\nVisualizing distributions of categorical variables similarly except using geom_bar() to yeild bar plots for binary and ordinal variables in the dataset.\n\n# Reshape the dataframe to long format\ndf_long_fact &lt;- gather(diabetes_processed |&gt;\n                    select(-Diabetes_binary) |&gt;     \n                    select(where(is.factor)), key = \"variable\", value = \"value\")\n\n# Create the combined histogram plot\nggplot(df_long_fact, aes(x = as.factor(as.numeric(value)))) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45,\n                                   vjust = 1,\n                                   hjust = 1,\n                                   size = 4) ) +\n  facet_wrap(~ variable, scales = \"free\",ncol = 4) +\n  ggtitle(\"BarChart of all Categorical Predictors\") +\n  xlab(\"Categories\")+\n  ylab(\"Frequency\")\n\n\n\n\n\n\nBivariate Analysis\nExploring the relationship of predictor variables with response:\n\nHelps in selecting and prioritizing predictor variables with strong associations with the response variable\nDetects patterns or trends in the data, such as linear or non-linear relationships, which can guide the choice of models and transformations.\\\n\nVisualizing relationships of categorical variables by faceted stacked bar plots. To do this, we create a long data set using pivot_longer() . Then we create stacked bar plots using fill argument within aes() and use geom_bar() . This visualizes the proportion of response in each group/category of the corresponding predictor variables.\n\n#Select Categorical Variables\ndf_pred &lt;- diabetes_processed |&gt;\n  select(-c(Diabetes_binary,BMI))\n\n#Create Long Dataset\ndf_long_bi &lt;- diabetes_processed |&gt;\n  pivot_longer(cols = colnames(df_pred),names_to = \"variables\",values_to = \"values\")\n\n\n\n# Create the faceted stacked bar plot\nggplot(df_long_bi, aes(x = values, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ variables, scales = \"free_x\", nrow = 5) +\n  theme_minimal() +\n  labs(title = \"Bivariate Faceted Stacked Bar Plots\",\n       x = \"Predictor Value\",\n       y = \"Proportion\")\n\n\n\n\nVisualizing relationships of continuous variable (only one in this dataset by faceted jitter plots. We perform similar steps as above but use geom_jitter() for jitters plots. This visualizes the proportion of response in an interval of continuous predictors (BMI here).\n\n#Select continous variable and create long dataset\ndf_long_cont &lt;- diabetes_processed |&gt;\n  pivot_longer(cols = BMI,names_to = \"variables\",values_to = \"values\")\n\n\n\n# Create the jitter plot\nggplot(df_long_cont, aes(x = values, y = Diabetes_binary)) +\n  geom_jitter(width = 0.2) +\n  facet_wrap(~ variables, scales = \"free_y\") +\n  theme_minimal() +\n  labs(title = \"Bivariate Jitter Plot with Binary Response\",\n       x = \"Response\",\n       y = \"Predictor Value\")\n\n\n\n\n\n\nCorrelation Between Predictors\nYielding a Covariance Matrix (Without Response) to determine if some variables vary together. This helps with variable selection without loosing too much variance if there is a strong evidence of multicolinearity. For this we utilize cor() from R base to generate a correlation matrix.\n\n#Creating a Correlation Matrix\ndiabetes_cor &lt;- round(\n  cor(diabetes_data |&gt;\n    select(-Diabetes_binary),\n  method = \"spearman\"),2)\nhead(diabetes_cor)\n\n                     HighBP HighChol  BMI Smoker Stroke HeartDiseaseorAttack\nHighBP                 1.00     0.30 0.24   0.10   0.13                 0.21\nHighChol               0.30     1.00 0.14   0.09   0.09                 0.18\nBMI                    0.24     0.14 1.00   0.02   0.02                 0.06\nSmoker                 0.10     0.09 0.02   1.00   0.06                 0.11\nStroke                 0.13     0.09 0.02   0.06   1.00                 0.20\nHeartDiseaseorAttack   0.21     0.18 0.06   0.11   0.20                 1.00\n                     PhysActivity Fruits Veggies HvyAlcoholConsump\nHighBP                      -0.13  -0.04   -0.06              0.00\nHighChol                    -0.08  -0.04   -0.04             -0.01\nBMI                         -0.14  -0.10   -0.07             -0.05\nSmoker                      -0.09  -0.08   -0.03              0.10\nStroke                      -0.07  -0.01   -0.04             -0.02\nHeartDiseaseorAttack        -0.09  -0.02   -0.04             -0.03\n                     AnyHealthcare NoDocbcCost GenHlth DiffWalk  Sex   Age\nHighBP                        0.04        0.02    0.30     0.22 0.05  0.34\nHighChol                      0.04        0.01    0.21     0.14 0.03  0.27\nBMI                          -0.01        0.05    0.26     0.18 0.09 -0.02\nSmoker                       -0.02        0.05    0.16     0.12 0.09  0.12\nStroke                        0.01        0.03    0.16     0.18 0.00  0.13\nHeartDiseaseorAttack          0.02        0.03    0.24     0.21 0.09  0.23\n                     Education Income\nHighBP                   -0.14  -0.18\nHighChol                 -0.07  -0.09\nBMI                      -0.12  -0.09\nSmoker                   -0.17  -0.13\nStroke                   -0.07  -0.12\nHeartDiseaseorAttack     -0.10  -0.14\n\n\nReshaping the correlation matrix using reshape2::melt() to make data compatible for heatmap style plot from ggplot2 .\n\n#Reshaping to make it compatible for heatmap style graph\nmelted_cormat &lt;- melt(diabetes_cor)\n\n#creating a new column for use in interactive heatmap\nmelted_cormat$text &lt;- paste0(\"Var1: \",\n                            melted_cormat$Var1,\n                            \", \" ,\n                            \"Var2: \",\n                            melted_cormat$Var2,\n                            \", \",\n                            \"Corr: \",\n                            melted_cormat$value)\nhead(melted_cormat)\n\n                  Var1   Var2 value\n1               HighBP HighBP  1.00\n2             HighChol HighBP  0.30\n3                  BMI HighBP  0.24\n4               Smoker HighBP  0.10\n5               Stroke HighBP  0.13\n6 HeartDiseaseorAttack HighBP  0.21\n                                                  text\n1                  Var1: HighBP, Var2: HighBP, Corr: 1\n2              Var1: HighChol, Var2: HighBP, Corr: 0.3\n3                  Var1: BMI, Var2: HighBP, Corr: 0.24\n4                Var1: Smoker, Var2: HighBP, Corr: 0.1\n5               Var1: Stroke, Var2: HighBP, Corr: 0.13\n6 Var1: HeartDiseaseorAttack, Var2: HighBP, Corr: 0.21\n\n\nVisualizing the correlation matrix using geom_tile() .\n\nplt &lt;- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value,text = text))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", \n                      high = \"red\", \n                      mid = \"white\", \n                      midpoint = 0, \n                      limit = c(-1,1), \n                      space = \"Lab\", \n                      name=\"Correlation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45,\n                                   vjust = 1,\n                                   hjust = 1)) +\n  coord_fixed() +\n  labs(title = \"Correlation Matrix for Predictors (Ranked)\",\n       x = \"Variables Horizontal\",\n       y = \"Variables Vertical\")\n  \nplt\n\n\n\n\nCreating an interactive plot using plotly::ggplotly()\n\nggplotly(plt,tooltip = \"text\")\n\n\n\n\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modelling.html",
    "href": "Modelling.html",
    "title": "Modelling",
    "section": "",
    "text": "Model Selection and Tuning for Diabetes Dataset."
  },
  {
    "objectID": "Modelling.html#modelling-metric-logloss",
    "href": "Modelling.html#modelling-metric-logloss",
    "title": "Modelling",
    "section": "Modelling Metric: Logloss",
    "text": "Modelling Metric: Logloss\n\nWhat is Logloss?\nLog loss is a way to measure how well a model predicts probabilities, especially for binary outcomes (like predicting if someone has diabetes or not). It measures the performance of a classification model by evaluating the uncertainty of the predicted probabilities. The farther the prediction probability is from the actual value, higher is its log-loss value.\n\n\nAdvantages over Accuracy\n\nUnlike accuracy, log loss takes into account the predicted probabilities. It penalizes confident wrong predictions more than less confident ones, thus encouraging well-calibrated probabilities\nAccuracy can be misleading in datasets where simply predicting the majority class can yield high accuracy but poor performance. Log loss, however, considers the predicted probabilities for both classes, providing a better evaluation"
  },
  {
    "objectID": "Modelling.html#model-description",
    "href": "Modelling.html#model-description",
    "title": "Modelling",
    "section": "Model Description",
    "text": "Model Description\nLogistic regression is a statistical method used to model the relationship between a binary response variable and one or more predictor variables (can be continuous, binary, ordinal or other categorical type). It uses the logistic function to model probabilities and applies the GLM framework where the link function connects the mean of the distribution of the outcome variable to a linear combination of the predictors. For logistic regression, the link function is the logit function . It can handle continuous, binary, and ordinal predictors (using dummies) effectively, making it a good choice for various types of data in predicting binary outcomes."
  },
  {
    "objectID": "Modelling.html#model-selection",
    "href": "Modelling.html#model-selection",
    "title": "Modelling",
    "section": "Model Selection",
    "text": "Model Selection\nWe will have three different fits for logistic regression type. The variable selected for each models are based on type of predictors as follows:\n\nEach model will include the Base variables (“Age”, “Sex”, “BMI” and “GenHlth”) and a cluster of variables grouped by classes. These classes are:\n\nCondition: “HighBP”, “HighChol”, “Stroke”, “HeartDiseaseorAttack” and “DiffWalk”\nHabits (Good and Bad): “Smoker”, “HvyAlcoholConsump”, “PhysActivity”, “Fruits” and “Veggies”\nEconomic Circumstances: “AnyHealthcare”, “NoDocbcCost”, “Education” and “Income”\n\n\nModel 1 will fit ~Base Variables + Condition Variables.\n\nModel 2 will fit ~ Base Variables + Habits Variables.\n\nModel 3 will fit ~ Base Variables + Economic Circumstances Variables.\n\nThe best model be chosen using CV with log-loss as the metric."
  },
  {
    "objectID": "Modelling.html#model-fit",
    "href": "Modelling.html#model-fit",
    "title": "Modelling",
    "section": "Model Fit",
    "text": "Model Fit\nSelecting the columns for each fit based on designated variable classes\n\n#Colnames based on Variable classes\nbase_cols &lt;- c(\"Age\", \"Sex\", \"BMI\", \"GenHlth\")\ncond_cols &lt;- c(\"HighBP\", \"HighChol\", \"Stroke\", \"HeartDiseaseorAttack\", \"DiffWalk\")\nhabit_cols &lt;- c(\"Smoker\", \"HvyAlcoholConsump\", \"PhysActivity\", \"Fruits\", \"Veggies\")\necon_cols &lt;- c(\"AnyHealthcare\", \"NoDocbcCost\", \"Education\", \"Income\")\n\n#Ensuring that the levels of the factor are  valid names for model\ntrain$Diabetes_binary &lt;- make.names(as.factor(train$Diabetes_binary))\ntest$Diabetes_binary &lt;- make.names(as.factor(test$Diabetes_binary))\n\n# Trainsets based on different models\ntrain_1 &lt;- train |&gt;\n  select(all_of(base_cols), all_of(cond_cols),Diabetes_binary)\n\ntrain_2 &lt;- train |&gt;\n  select(all_of(base_cols), all_of(habit_cols),Diabetes_binary)\n\ntrain_3 &lt;- train |&gt;\n  select(all_of(base_cols), all_of(econ_cols),Diabetes_binary)\n\nDefining the CrossVal criteria (5 Fold with 3 repeats) with trainControl() .\n\n#Defining CrossVal criteria\ntrctrl &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 5,\n  repeats = 3,\n  summaryFunction = mnLogLoss,\n  classProbs = TRUE)\n\nFitting Models using train() .\n\n#Model 1\nlogreg_fit1 &lt;- train(Diabetes_binary ~., data = train_1, method = \"glm\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n family = \"binomial\",\n metric = \"logLoss\",\n tuneLength = 10\n )\n\n#Model2\nlogreg_fit2 &lt;- train(Diabetes_binary ~., data = train_2, method = \"glm\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n family = \"binomial\",\n metric = \"logLoss\",\n tuneLength = 10\n )\n\n\n#Model 3\nlogreg_fit3 &lt;- train(Diabetes_binary ~., data = train_3, method = \"glm\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n family = \"binomial\",\n metric = \"logLoss\",\n tuneLength = 10\n )\n\nEvaluating the Models\n\n#Usable format for us\ntest_metrics &lt;- as.numeric(substr(test$Diabetes_binary,2,2))\n\n#predictions\npred_lr1 &lt;- predict(logreg_fit1,newdata = test, type = \"prob\")\npred_lr2 &lt;- predict(logreg_fit2,newdata = test, type = \"prob\")\npred_lr3 &lt;- predict(logreg_fit3,newdata = test, type = \"prob\")\n\n#metrics\npaste(\"Model 1 logLoss: \",logLoss(test_metrics, pred_lr1$X1));\n\n[1] \"Model 1 logLoss:  0.321363326575826\"\n\npaste(\"Model 2 logLoss: \",logLoss(test_metrics, pred_lr2$X1));\n\n[1] \"Model 2 logLoss:  0.33218531831637\"\n\npaste(\"Model 3 logLoss: \",logLoss(test_metrics, pred_lr3$X1));\n\n[1] \"Model 3 logLoss:  0.3321042023893\"\n\n\nBased on the logLoss metric, logreg_fit1 is the best model from Logistic regression type models with 0.32136.\n\n\n#Model\nlogreg_fit1\n\nGeneralized Linear Model \n\n177577 samples\n     9 predictor\n     2 classes: 'X0', 'X1' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 142062, 142061, 142062, 142062, 142061, 142063, ... \nResampling results:\n\n  logLoss  \n  0.3195697"
  },
  {
    "objectID": "Modelling.html#model-selection-1",
    "href": "Modelling.html#model-selection-1",
    "title": "Modelling",
    "section": "Model Selection",
    "text": "Model Selection\nWe will be using same variables as Model 1 for consistency when comparing classes of models"
  },
  {
    "objectID": "Modelling.html#model-fit-1",
    "href": "Modelling.html#model-fit-1",
    "title": "Modelling",
    "section": "Model Fit",
    "text": "Model Fit\nWe will use train_1 which includes the predictors corresponding to chosen variables. We will use the same CrossVal criteria as defined in trctrl . Creating tree_grid1 associated with method = \"rpart\" for parameter tuning, Using train() to train the model with a grid search to find the optimal parameters that minimizes logloss\n\n# Defining a tuning grid for a rpart model\ntree_grid1 &lt;- expand.grid(cp = seq(0, 0.2, by = 0.002))\n\n# Training the model using rpart method\ntree_fit &lt;- train(Diabetes_binary ~.,\n                   data = train_1, \n                   method = \"rpart\",\n                   trControl = trctrl,\n                   preProcess = c(\"center\", \"scale\"),\n                   tuneGrid = tree_grid1,\n                   metric = \"logLoss\")\n\nEvaluating the Model\n\npred_tr &lt;- predict(tree_fit,newdata = test, type = \"prob\")\n\npaste(\"Tree Classifier logLoss: \",logLoss(test_metrics, pred_tr$X1))\n\n[1] \"Tree Classifier logLoss:  Inf\"\n\n\nThe logloss for tree_fit is infitnity.\n\n#Model\ntree_fit\n\nCART \n\n177577 samples\n     9 predictor\n     2 classes: 'X0', 'X1' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 142061, 142062, 142062, 142062, 142061, 142061, ... \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3466719\n  0.002  0.3563038\n  0.004  0.3564215\n  0.006  0.4007540\n  0.008  0.4037576\n  0.010  0.4037576\n  0.012  0.4037576\n  0.014  0.4037576\n  0.016  0.4037576\n  0.018  0.4037576\n  0.020  0.4037576\n  0.022  0.4037576\n  0.024  0.4037576\n  0.026  0.4037576\n  0.028  0.4037576\n  0.030  0.4037576\n  0.032  0.4037576\n  0.034  0.4037576\n  0.036  0.4037576\n  0.038  0.4037576\n  0.040  0.4037576\n  0.042  0.4037576\n  0.044  0.4037576\n  0.046  0.4037576\n  0.048  0.4037576\n  0.050  0.4037576\n  0.052  0.4037576\n  0.054  0.4037576\n  0.056  0.4037576\n  0.058  0.4037576\n  0.060  0.4037576\n  0.062  0.4037576\n  0.064  0.4037576\n  0.066  0.4037576\n  0.068  0.4037576\n  0.070  0.4037576\n  0.072  0.4037576\n  0.074  0.4037576\n  0.076  0.4037576\n  0.078  0.4037576\n  0.080  0.4037576\n  0.082  0.4037576\n  0.084  0.4037576\n  0.086  0.4037576\n  0.088  0.4037576\n  0.090  0.4037576\n  0.092  0.4037576\n  0.094  0.4037576\n  0.096  0.4037576\n  0.098  0.4037576\n  0.100  0.4037576\n  0.102  0.4037576\n  0.104  0.4037576\n  0.106  0.4037576\n  0.108  0.4037576\n  0.110  0.4037576\n  0.112  0.4037576\n  0.114  0.4037576\n  0.116  0.4037576\n  0.118  0.4037576\n  0.120  0.4037576\n  0.122  0.4037576\n  0.124  0.4037576\n  0.126  0.4037576\n  0.128  0.4037576\n  0.130  0.4037576\n  0.132  0.4037576\n  0.134  0.4037576\n  0.136  0.4037576\n  0.138  0.4037576\n  0.140  0.4037576\n  0.142  0.4037576\n  0.144  0.4037576\n  0.146  0.4037576\n  0.148  0.4037576\n  0.150  0.4037576\n  0.152  0.4037576\n  0.154  0.4037576\n  0.156  0.4037576\n  0.158  0.4037576\n  0.160  0.4037576\n  0.162  0.4037576\n  0.164  0.4037576\n  0.166  0.4037576\n  0.168  0.4037576\n  0.170  0.4037576\n  0.172  0.4037576\n  0.174  0.4037576\n  0.176  0.4037576\n  0.178  0.4037576\n  0.180  0.4037576\n  0.182  0.4037576\n  0.184  0.4037576\n  0.186  0.4037576\n  0.188  0.4037576\n  0.190  0.4037576\n  0.192  0.4037576\n  0.194  0.4037576\n  0.196  0.4037576\n  0.198  0.4037576\n  0.200  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0."
  },
  {
    "objectID": "Modelling.html#model-selection-2",
    "href": "Modelling.html#model-selection-2",
    "title": "Modelling",
    "section": "Model Selection",
    "text": "Model Selection\nWe will be using same variables as Model 1 for consistency when comparing classes of models"
  },
  {
    "objectID": "Modelling.html#model-fit-2",
    "href": "Modelling.html#model-fit-2",
    "title": "Modelling",
    "section": "Model Fit",
    "text": "Model Fit\nWe will use train_1 which includes the predictors corresponding to chosen variables. We will useCrossVal criteria as defined in trctrl_rf due to long runtime.\n\n#Defining CrossVal criteria\ntrctrl_rf &lt;- trainControl(\n  method = \"repeatedcv\",\n  number = 3,\n  repeats = 3,\n  summaryFunction = mnLogLoss,\n  classProbs = TRUE)\n\nCreating rf_grid associated with method = \"ranger\" for parameter tuning, Using train() to train the model with a grid search to find the optimal parameters that minimizes logloss\n\n# Defining a tuning grid for rf- model.\nrf_grid &lt;- expand.grid(\n  .mtry = seq(1, length(colnames(train_1))-1),\n  .splitrule= \"extratrees\",\n  .min.node.size = c(10, 20))\n\n# Training the model using random forest method\nrf_fit &lt;- train(Diabetes_binary ~., \n                data = train_1, \n                method = \"ranger\",\n                trControl = trctrl,\n                num.trees = 100,\n                preProcess = c(\"center\", \"scale\"),\n                tuneGrid = rf_grid,\n                metric = \"logLoss\")\n\n# Model Details\nrf_fit\n\nRandom Forest \n\n177577 samples\n     9 predictor\n     2 classes: 'X0', 'X1' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 142061, 142061, 142062, 142061, 142063, 142062, ... \nResampling results across tuning parameters:\n\n  mtry  min.node.size  logLoss  \n  1     10             0.3574442\n  1     20             0.3572843\n  2     10             0.3343535\n  2     20             0.3346149\n  3     10             0.3261652\n  3     20             0.3260256\n  4     10             0.3227547\n  4     20             0.3227274\n  5     10             0.3213611\n  5     20             0.3212403\n  6     10             0.3207856\n  6     20             0.3205333\n  7     10             0.3207602\n  7     20             0.3202356\n  8     10             0.3210045\n  8     20             0.3202838\n  9     10             0.3214757\n  9     20             0.3206123\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 7, splitrule = extratrees\n and min.node.size = 20.\n\n\nEvaluating the Model\n\npred_rf &lt;- predict(rf_fit,newdata = test, type = \"prob\")\n\npaste(\"Random Forest logLoss: \",logLoss(test_metrics, pred_rf$X1))\n\n[1] \"Random Forest logLoss:  0.321534238452863\"\n\n\nThe logloss for rf_fit is 0.32143.\n\nrf_fit\n\nRandom Forest \n\n177577 samples\n     9 predictor\n     2 classes: 'X0', 'X1' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 142061, 142061, 142062, 142061, 142063, 142062, ... \nResampling results across tuning parameters:\n\n  mtry  min.node.size  logLoss  \n  1     10             0.3574442\n  1     20             0.3572843\n  2     10             0.3343535\n  2     20             0.3346149\n  3     10             0.3261652\n  3     20             0.3260256\n  4     10             0.3227547\n  4     20             0.3227274\n  5     10             0.3213611\n  5     20             0.3212403\n  6     10             0.3207856\n  6     20             0.3205333\n  7     10             0.3207602\n  7     20             0.3202356\n  8     10             0.3210045\n  8     20             0.3202838\n  9     10             0.3214757\n  9     20             0.3206123\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 7, splitrule = extratrees\n and min.node.size = 20."
  }
]