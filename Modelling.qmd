---
title: "Modelling"
format: html
editor: visual
---

# Document Details

#### Author: *Smit Miyani*

#### Collaborators: *N/A*

#### Assignment: Final Project - Modelling

#### Date: *26JUL24*

#### Purpose

*Model Selection and Tuning for Diabetes Dataset.*

# Introduction

The `diabetes_binary_health_indicators_BRFSS2015.csv` dataset from Kaggle contains health data from the BRFSS 2015 survey, focusing on the relationship between various health indicators and diabetes. The response variable, `Diabetes_binary`, indicates the presence of diabetes (1) or not (0).

### Selected Variables

Key variables used for modeling include:

-   **Diabetes_binary**: Response variable (1 for diabetes, 0 for no diabetes).
-   **HighBP**, **HighChol**, **Smoker**, **Stroke**, **HeartDiseaseorAttack**, **PhysActivity**, **Fruits**, **Veggies**, **HvyAlcoholConsump**, **AnyHealthcare**, **NoDocbcCost**, **DiffWalk**, **Sex**: Binary variables (1 for Yes, 0 for No).
-   **BMI**: Continuous variable.
-   **GenHlth**, **Age**, **Education**, **Income**: Ordinal variables.

### Modeling Approach

To predict the presence of diabetes based on these health indicators, we will employ three different modeling techniques:

1.  **Logistic Regression**: A statistical model that estimates the probability of a binary outcome based on one or more predictor variables.

2.  **Classification Tree**: A decision tree algorithm that splits the data into subsets based on the value of input features to predict the outcome.

3.  **Random Forest**: An ensemble learning method that constructs multiple decision trees and merges them to produce a more accurate and stable prediction.

The goal is to select the best model based on predictive performance for deployment in an API.

## Modelling Metric: Logloss

### What is Logloss?

Log loss is a way to measure how well a model predicts probabilities, especially for binary outcomes (like predicting if someone has diabetes or not). It measures the performance of a classification model by evaluating the uncertainty of the predicted probabilities. The farther the prediction probability is from the actual value, higher is its log-loss value.

### Advantages over Accuracy

-   Unlike accuracy, log loss takes into account the predicted probabilities. It penalizes confident wrong predictions more than less confident ones, thus encouraging well-calibrated probabilities

-   Accuracy can be misleading in datasets where simply predicting the majority class can yield high accuracy but poor performance. Log loss, however, considers the predicted probabilities for both classes, providing a better evaluation

# Data Split

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

#Remove Warnings
```



```{r}
#Packages
library(tidyverse)
library(caret)
library(ggplot2)
library(plotly)
library(reshape2)
library(caret)
library(Metrics)
library(ranger)

diabetes_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

diabetes_data <- diabetes_data |>
  select(-c(CholCheck,MentHlth,PhysHlth))

diabetes_processed <- diabetes_data |>
  #Converts all columns to factors
  mutate(across(everything(),as.factor)) |>
  #Converts the only non-categorical variable back to numeric 
  mutate(BMI = as.numeric(BMI))
```

Splitting the data in test set and train set using `createDataPartition()`. All the models will be tuned with Cross Validation on `train` and evaluated on `test`.

```{r}
set.seed(333) #For Reproducibility 

split <- createDataPartition(y = diabetes_processed$Diabetes_binary, 
                             p = 0.7, 
                             list = FALSE)
train <- diabetes_processed[split, ]
test <- diabetes_processed[-split, ]

#Check size
dim(train) ; dim(test);
anyNA(diabetes_processed); #final check for NULL values
```

# Logistic Regression Models

## Model Description

Logistic regression is a statistical method used to model the relationship between a binary response variable and one or more predictor variables (can be continuous, binary, ordinal or other categorical type). It uses the logistic function to model probabilities and applies the GLM framework where the link function connects the mean of the distribution of the outcome variable to a linear combination of the predictors. For logistic regression, the link function is the logit function . It can handle continuous, binary, and ordinal predictors (using dummies) effectively, making it a good choice for various types of data in predicting binary outcomes.

## Model Selection

We will have three different fits for logistic regression type. The variable selected for each models are based on type of predictors as follows:

1.  Each model will include the Base variables ("Age", "Sex", "BMI" and "GenHlth") and a cluster of variables grouped by classes. These classes are:

    1.  Condition: "HighBP", "HighChol", "Stroke", "HeartDiseaseorAttack" and "DiffWalk"

    2.  Habits (Good and Bad): "Smoker", "HvyAlcoholConsump", "PhysActivity", "Fruits" and "Veggies"

    3.  Economic Circumstances: "AnyHealthcare", "NoDocbcCost", "Education" and "Income"\

2.  Model 1 will fit \~Base Variables + Condition Variables.\

3.  Model 2 will fit \~ Base Variables + Habits Variables.\

4.  Model 3 will fit \~ Base Variables + Economic Circumstances Variables.\

5.  The best model be chosen using CV with log-loss as the metric.

## Model Fit

Selecting the columns for each fit based on designated variable classes

```{r}
#Colnames based on Variable classes
base_cols <- c("Age", "Sex", "BMI", "GenHlth")
cond_cols <- c("HighBP", "HighChol", "Stroke", "HeartDiseaseorAttack", "DiffWalk")
habit_cols <- c("Smoker", "HvyAlcoholConsump", "PhysActivity", "Fruits", "Veggies")
econ_cols <- c("AnyHealthcare", "NoDocbcCost", "Education", "Income")

#Ensuring that the levels of the factor are  valid names for model
train$Diabetes_binary <- make.names(as.factor(train$Diabetes_binary))
test$Diabetes_binary <- make.names(as.factor(test$Diabetes_binary))

# Trainsets based on different models
train_1 <- train |>
  select(all_of(base_cols), all_of(cond_cols),Diabetes_binary)

train_2 <- train |>
  select(all_of(base_cols), all_of(habit_cols),Diabetes_binary)

train_3 <- train |>
  select(all_of(base_cols), all_of(econ_cols),Diabetes_binary)

```

Defining the CrossVal criteria (5 Fold with 3 repeats) with `trainControl()` .

```{r}
#Defining CrossVal criteria
trctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  summaryFunction = mnLogLoss,
  classProbs = TRUE)
```

Fitting Models using `train()` .

```{r}

#Model 1
logreg_fit1 <- train(Diabetes_binary ~., data = train_1, method = "glm",
 trControl=trctrl,
 preProcess = c("center", "scale"),
 family = "binomial",
 metric = "logLoss",
 tuneLength = 10
 )

#Model2
logreg_fit2 <- train(Diabetes_binary ~., data = train_2, method = "glm",
 trControl=trctrl,
 preProcess = c("center", "scale"),
 family = "binomial",
 metric = "logLoss",
 tuneLength = 10
 )


#Model 3
logreg_fit3 <- train(Diabetes_binary ~., data = train_3, method = "glm",
 trControl=trctrl,
 preProcess = c("center", "scale"),
 family = "binomial",
 metric = "logLoss",
 tuneLength = 10
 )

```

Evaluating the Models

```{r}
#Usable format for us
test_metrics <- as.numeric(substr(test$Diabetes_binary,2,2))

#predictions
pred_lr1 <- predict(logreg_fit1,newdata = test, type = "prob")
pred_lr2 <- predict(logreg_fit2,newdata = test, type = "prob")
pred_lr3 <- predict(logreg_fit3,newdata = test, type = "prob")

#metrics
paste("Model 1 logLoss: ",logLoss(test_metrics, pred_lr1$X1));
paste("Model 2 logLoss: ",logLoss(test_metrics, pred_lr2$X1));
paste("Model 3 logLoss: ",logLoss(test_metrics, pred_lr3$X1));

```

Based on the logLoss metric, `logreg_fit1` is the best model from Logistic regression type models with 0.32136.\

```{r}
#Model
logreg_fit1
```

# Classification Tree

A classification tree model is a decision tree used for classifying data into distinct categories. It splits the data into subsets based on the values of predictor variables, creating a tree-like structure where each internal node represents a decision based on one predictor, and each leaf node represents a class label. The goal is to partition the data into groups that are as homogeneous as possible with respect to the target variable. Classification trees are straightforward to interpret and visualize, making them useful for understanding model decisions and relationships between variables. They also account for interaction between variable, making variable selection easier.

## Model Selection

We will be using same variables as Model 1 for consistency when comparing classes of models

## Model Fit

We will use `train_1` which includes the predictors corresponding to chosen variables. We will use the same CrossVal criteria as defined in `trctrl` . Creating `tree_grid1` associated with `method = "rpart"` for parameter tuning, Using `train()` to train the model with a grid search to find the optimal parameters that minimizes logloss

```{r}
# Defining a tuning grid for a rpart model
tree_grid1 <- expand.grid(cp = seq(0, 0.2, by = 0.002))

# Training the model using rpart method
tree_fit <- train(Diabetes_binary ~.,
                   data = train_1, 
                   method = "rpart",
                   trControl = trctrl,
                   preProcess = c("center", "scale"),
                   tuneGrid = tree_grid1,
                   metric = "logLoss")
```

Evaluating the Model

```{r}
pred_tr <- predict(tree_fit,newdata = test, type = "prob")

paste("Tree Classifier logLoss: ",logLoss(test_metrics, pred_tr$X1))
```

The logloss for `tree_fit` is infitnity.

```{r}
#Model
tree_fit
```

# Random Forest

A random forest is an ensemble learning method that combines multiple classification trees to improve predictive accuracy and robustness. It builds many decision trees (usually hundreds) during training and outputs the mode of the classes (for classification) from all the trees. Random forests enhance prediction by averaging the results of multiple trees, reducing the risk of overfitting compared to a single decision tree. This method improves performance and generalization, making it a powerful tool for complex datasets and avoiding the limitations of a basic classification tree.

## Model Selection

We will be using same variables as Model 1 for consistency when comparing classes of models

## Model Fit

We will use `train_1` which includes the predictors corresponding to chosen variables. We will useCrossVal criteria as defined in `trctrl_rf` due to long runtime.

```{r}
#Defining CrossVal criteria
trctrl_rf <- trainControl(
  method = "repeatedcv",
  number = 3,
  repeats = 3,
  summaryFunction = mnLogLoss,
  classProbs = TRUE)
```

Creating `rf_grid` associated with `method = "ranger"` for parameter tuning, Using `train()` to train the model with a grid search to find the optimal parameters that minimizes logloss

```{r}
# Defining a tuning grid for rf- model.
rf_grid <- expand.grid(
  .mtry = seq(1, length(colnames(train_1))-1),
  .splitrule= "extratrees",
  .min.node.size = c(10, 20))

# Training the model using random forest method
rf_fit <- train(Diabetes_binary ~., 
                data = train_1, 
                method = "ranger",
                trControl = trctrl,
                num.trees = 100,
                preProcess = c("center", "scale"),
                tuneGrid = rf_grid,
                metric = "logLoss")

# Model Details
rf_fit
```

Evaluating the Model

```{r}
pred_rf <- predict(rf_fit,newdata = test, type = "prob")

paste("Random Forest logLoss: ",logLoss(test_metrics, pred_rf$X1))
```

The logloss for `rf_fit` is 0.32143.

```{r}
rf_fit
```

# Best Model

The best fit model is `logreg_fit1` with logloss of 0.32136. However the logloss of `rf_fit` 0.32143 which is very close second. On a different test set the it is not unlikely that the `rf_fit` model generalizes better. It is difficult to tell them apart.

```{r}
logreg_fit1
```
